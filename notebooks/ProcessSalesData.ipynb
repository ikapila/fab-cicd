{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a162f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Configuration\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, avg, to_date\n",
    "\n",
    "# Environment-specific parameters (will be substituted during deployment)\n",
    "storage_account = \"{{storage_account}}\"\n",
    "data_lake_path = \"{{data_lake_path}}\"\n",
    "log_level = \"{{log_level}}\"\n",
    "\n",
    "print(f\"Storage Account: {storage_account}\")\n",
    "print(f\"Data Lake Path: {data_lake_path}\")\n",
    "print(f\"Log Level: {log_level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Raw Sales Data\n",
    "raw_sales_path = f\"{data_lake_path}raw/sales.csv\"\n",
    "\n",
    "df_sales = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(raw_sales_path)\n",
    "\n",
    "print(f\"Loaded {df_sales.count()} sales records\")\n",
    "df_sales.printSchema()\n",
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Transformation\n",
    "# Clean and transform the data\n",
    "df_cleaned = df_sales \\\n",
    "    .filter(col(\"amount\") > 0) \\\n",
    "    .filter(col(\"quantity\") > 0) \\\n",
    "    .withColumn(\"sale_date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"unit_price\", col(\"amount\") / col(\"quantity\"))\n",
    "\n",
    "print(f\"Cleaned data: {df_cleaned.count()} records\")\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create Aggregations\n",
    "# Regional sales summary\n",
    "df_regional_summary = df_cleaned.groupBy(\"region\", \"sale_date\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_sales\"),\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"region\", \"sale_date\")\n",
    "\n",
    "print(\"Regional Sales Summary:\")\n",
    "df_regional_summary.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Product category analysis\n",
    "df_category_summary = df_cleaned.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_sales\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        avg(\"unit_price\").alias(\"avg_unit_price\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_sales\").desc())\n",
    "\n",
    "print(\"Product Category Summary:\")\n",
    "df_category_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Write Processed Data\n",
    "# Save regional summary\n",
    "processed_path = f\"{data_lake_path}processed/sales_regional_summary\"\n",
    "df_regional_summary.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(processed_path)\n",
    "\n",
    "print(f\"Regional summary saved to: {processed_path}\")\n",
    "\n",
    "# Save category summary\n",
    "category_path = f\"{data_lake_path}processed/sales_category_summary\"\n",
    "df_category_summary.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(category_path)\n",
    "\n",
    "print(f\"Category summary saved to: {category_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d507b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Data Quality Checks\n",
    "total_amount = df_cleaned.agg(sum(\"amount\")).collect()[0][0]\n",
    "record_count = df_cleaned.count()\n",
    "null_count = df_cleaned.filter(col(\"amount\").isNull()).count()\n",
    "\n",
    "print(f\"Data Quality Report:\")\n",
    "print(f\"  Total Records: {record_count}\")\n",
    "print(f\"  Total Sales Amount: ${total_amount:,.2f}\")\n",
    "print(f\"  Null Amount Records: {null_count}\")\n",
    "print(f\"  Data Quality Score: {((record_count - null_count) / record_count * 100):.2f}%\")\n",
    "\n",
    "# Assert quality thresholds\n",
    "assert null_count == 0, \"Found null values in amount column\"\n",
    "assert record_count > 0, \"No records to process\"\n",
    "\n",
    "print(\"\\nâœ… All data quality checks passed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
