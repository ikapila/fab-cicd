{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed439e01",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "from pyspark.sql import SparkSession\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39007a5",
   "metadata": {},
   "source": [
    "## 2. Access Environment Variables\n",
    "\n",
    "Use `mssparkutils.env.getVariable()` to retrieve values from the active Variable Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment-specific variables\n",
    "storage_account = mssparkutils.env.getVariable(\"storage_account\")\n",
    "api_endpoint = mssparkutils.env.getVariable(\"api_endpoint\")\n",
    "data_lake_path = mssparkutils.env.getVariable(\"data_lake_path\")\n",
    "log_level = mssparkutils.env.getVariable(\"log_level\")\n",
    "\n",
    "# Integer variables\n",
    "batch_size = int(mssparkutils.env.getVariable(\"batch_size\"))\n",
    "max_retries = int(mssparkutils.env.getVariable(\"max_retries\"))\n",
    "timeout_seconds = int(mssparkutils.env.getVariable(\"timeout_seconds\"))\n",
    "\n",
    "# Boolean variables\n",
    "enable_caching = mssparkutils.env.getVariable(\"enable_caching\").lower() == \"true\"\n",
    "\n",
    "print(\"Environment Variables Loaded:\")\n",
    "print(f\"  Storage Account: {storage_account}\")\n",
    "print(f\"  API Endpoint: {api_endpoint}\")\n",
    "print(f\"  Data Lake Path: {data_lake_path}\")\n",
    "print(f\"  Log Level: {log_level}\")\n",
    "print(f\"  Batch Size: {batch_size}\")\n",
    "print(f\"  Max Retries: {max_retries}\")\n",
    "print(f\"  Timeout: {timeout_seconds}s\")\n",
    "print(f\"  Caching Enabled: {enable_caching}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cbe0f",
   "metadata": {},
   "source": [
    "## 3. Configure Logging with Variable\n",
    "\n",
    "Use the `log_level` variable to configure logging dynamically per environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8668e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging based on environment\n",
    "log_level_map = {\n",
    "    \"DEBUG\": logging.DEBUG,\n",
    "    \"INFO\": logging.INFO,\n",
    "    \"WARNING\": logging.WARNING,\n",
    "    \"ERROR\": logging.ERROR\n",
    "}\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=log_level_map.get(log_level, logging.INFO),\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Logging configured at {log_level} level\")\n",
    "logger.debug(\"Debug messages visible in DEV only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45669297",
   "metadata": {},
   "source": [
    "## 4. Use Variables in Data Processing\n",
    "\n",
    "Apply batch size and storage path variables in Spark operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"VariableLibraryExample\").getOrCreate()\n",
    "\n",
    "# Configure Spark with caching based on variable\n",
    "if enable_caching:\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    logger.info(\"Spark caching enabled\")\n",
    "\n",
    "# Create sample data\n",
    "data = [(i, f\"name_{i}\", i * 10) for i in range(1, 10001)]\n",
    "columns = [\"id\", \"name\", \"value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Apply batch size from variable\n",
    "df_limited = df.limit(batch_size)\n",
    "\n",
    "logger.info(f\"Processing {df_limited.count()} rows (batch_size={batch_size})\")\n",
    "df_limited.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eafa51",
   "metadata": {},
   "source": [
    "## 5. Read from Data Lake Using Variable Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4488a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct path using variable\n",
    "# Note: In real scenario, this would read from actual data lake\n",
    "# This example shows the pattern\n",
    "\n",
    "try:\n",
    "    # Example: Read parquet from data lake\n",
    "    # df_lake = spark.read.parquet(f\"{data_lake_path}raw/sales/\")\n",
    "    \n",
    "    logger.info(f\"Would read from: {data_lake_path}raw/sales/\")\n",
    "    logger.info(f\"Storage account: {storage_account}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.warning(f\"Data lake read example (not connected): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83df2d5",
   "metadata": {},
   "source": [
    "## 6. API Integration with Variable Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55235c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "def call_api_with_retry(endpoint: str, max_retries: int, timeout: int):\n",
    "    \"\"\"\n",
    "    Call API with retry logic using variables from Variable Library\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logger.info(f\"API call attempt {attempt}/{max_retries}\")\n",
    "            \n",
    "            # Example API call (would need actual endpoint)\n",
    "            # response = requests.get(endpoint, timeout=timeout)\n",
    "            # response.raise_for_status()\n",
    "            \n",
    "            logger.info(f\"Would call: {endpoint} (timeout={timeout}s)\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {attempt} failed: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    logger.error(f\"All {max_retries} attempts failed\")\n",
    "    return False\n",
    "\n",
    "# Use variables in API call\n",
    "success = call_api_with_retry(\n",
    "    endpoint=api_endpoint,\n",
    "    max_retries=max_retries,\n",
    "    timeout=timeout_seconds\n",
    ")\n",
    "\n",
    "print(f\"API integration configured: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9133bdb",
   "metadata": {},
   "source": [
    "## 7. Dynamic Configuration Summary\n",
    "\n",
    "Show how variables enable environment-specific behavior without code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86224f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENVIRONMENT CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nStorage Configuration:\")\n",
    "print(f\"  Account: {storage_account}\")\n",
    "print(f\"  Path: {data_lake_path}\")\n",
    "\n",
    "print(f\"\\nAPI Configuration:\")\n",
    "print(f\"  Endpoint: {api_endpoint}\")\n",
    "print(f\"  Timeout: {timeout_seconds}s\")\n",
    "print(f\"  Max Retries: {max_retries}\")\n",
    "\n",
    "print(f\"\\nProcessing Configuration:\")\n",
    "print(f\"  Batch Size: {batch_size} rows\")\n",
    "print(f\"  Caching: {'Enabled' if enable_caching else 'Disabled'}\")\n",
    "print(f\"  Log Level: {log_level}\")\n",
    "\n",
    "print(f\"\\nEnvironment Detection:\")\n",
    "if 'dev' in storage_account.lower():\n",
    "    print(\"  Running in: DEVELOPMENT\")\n",
    "    print(\"  Characteristics: Debug logging, small batches, test endpoints\")\n",
    "elif 'uat' in storage_account.lower():\n",
    "    print(\"  Running in: UAT\")\n",
    "    print(\"  Characteristics: Info logging, medium batches, UAT endpoints\")\n",
    "elif 'prod' in storage_account.lower():\n",
    "    print(\"  Running in: PRODUCTION\")\n",
    "    print(\"  Characteristics: Warning logging, large batches, prod endpoints\")\n",
    "else:\n",
    "    print(\"  Running in: UNKNOWN\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ All variables loaded successfully from Variable Library\")\n",
    "print(\"✓ Configuration applied dynamically based on environment\")\n",
    "print(\"✓ No code changes needed between environments\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab27ff",
   "metadata": {},
   "source": [
    "## Key Benefits\n",
    "\n",
    "1. **Runtime Flexibility**: Change variables without redeploying notebooks\n",
    "2. **Environment Parity**: Same code runs in dev/uat/prod with different variables\n",
    "3. **Centralized Management**: Update variables in Fabric portal for all artifacts\n",
    "4. **Type Safety**: Explicit type conversion for Int and Bool variables\n",
    "5. **Separation of Concerns**: Code contains logic, Variable Library contains configuration\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Update your existing notebooks to use Variable Library variables\n",
    "- Replace hard-coded values with `mssparkutils.env.getVariable()` calls\n",
    "- Test in development environment\n",
    "- Promote to UAT/Production with environment-specific Variable Libraries"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
